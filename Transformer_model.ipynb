{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 20000\n",
    "maxlen = 200\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, sequence_length, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.positional_encoding = self.compute_positional_encoding(sequence_length, d_model)\n",
    "\n",
    "    def compute_positional_encoding(self, sequence_length, d_model):\n",
    "        positions = np.arange(sequence_length)[:, np.newaxis]\n",
    "        indices = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (indices // 2)) / np.float32(d_model))\n",
    "        angle_rads = positions * angle_rates\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        return tf.constant(angle_rads, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        self.dense = layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, query, key, value):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        Q = self.wq(query)  \n",
    "        K = self.wk(key)\n",
    "        V = self.wv(value)\n",
    "\n",
    "        Q = self.split_heads(Q, batch_size)  \n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        scaled_attention = tf.matmul(attention_weights, V)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return self.dense(concat_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='leaky_relu'),\n",
    "            layers.Dense(d_model),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.attention(inputs, inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, maxlen, d_model, num_heads, ff_dim):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = layers.Embedding(vocab_size, d_model)\n",
    "    x = embedding_layer(inputs)\n",
    "    x = PositionalEncoding(maxlen, d_model)(x)\n",
    "\n",
    "    transformer_block = TransformerBlock(d_model, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(20, activation='leaky_relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 200, 128)          2560000   \n",
      "                                                                 \n",
      " positional_encoding_2 (Posi  (None, 200, 128)         0         \n",
      " tionalEncoding)                                                 \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  (None, 200, 128)         99584     \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 20)                2580      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,662,185\n",
      "Trainable params: 2,662,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.5485 - accuracy: 0.6709 - val_loss: 0.3365 - val_accuracy: 0.8618\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.2355 - accuracy: 0.9057 - val_loss: 0.2802 - val_accuracy: 0.8860\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.1290 - accuracy: 0.9518 - val_loss: 0.3509 - val_accuracy: 0.8804\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.0571 - accuracy: 0.9807 - val_loss: 0.4532 - val_accuracy: 0.8680\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.0297 - accuracy: 0.9905 - val_loss: 0.7057 - val_accuracy: 0.8576\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.0205 - accuracy: 0.9930 - val_loss: 0.7996 - val_accuracy: 0.8502\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.0094 - accuracy: 0.9966 - val_loss: 0.8813 - val_accuracy: 0.8550\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 1.0816 - val_accuracy: 0.8152\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.9655 - val_accuracy: 0.8504\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 1.1510 - val_accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "d_model = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "\n",
    "\n",
    "model = build_model(num_words, maxlen, d_model, num_heads, ff_dim)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=32, workers=2, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2068 - accuracy: 0.8399\n",
      "Test Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
